---
title: "Homework 3"
author: "Wenxin Tian"
date: "`r Sys.Date()`"
output: github_document
---

```{r}
library(tidyverse)
```

## Problem 1:

```{r}
library(p8105.datasets)
data("instacart")
```

__Dataset description__:

The dataset shows `r nrow(instacart)` observations of `r ncol(instacart)` variables. Some key variables include product name and id, which aisle and department it was bought from, the order in which the item was added to cart, and whether the item was reordered, meaning it was ordered before by the same customer. For example, here are the first 5 products listed in the dataset:

```{r}
head(instacart, 5) |>
  knitr::kable()
```

__Questions:__

__1. How many aisles are there, and which aisles are the most items ordered from?__

```{r}
instacart |>
  group_by(aisle) |>
  summarize(count = n()) |>
  arrange(desc(count))
```

There are 134 aisles, and the top 5 aisles where the most items are ordered from are fresh vegetables, fresh fruits, packaged vegetable and fruits, yogurt, and packaged cheese.

__2. Make a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered. Arrange aisles sensibly, and organize your plot so others can read it.__

```{r}
instacart |>
  group_by(aisle) |>
  summarize(count = n()) |>
  filter(count >= 10000) |>
  arrange(desc(count)) |>
  mutate(aisle = fct_reorder(aisle, count)) |> # had to reorder the aisle factor
  ggplot(
    aes(x = count, y = aisle),
  ) +
  geom_col() +
  labs(
      title = "Popular Aisles",
      x = "Item Count",
      y = "Aisle"
    ) +
  theme_minimal()
```

__3. Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. Include the number of times each item is ordered in your table.__

```{r}
baking = 
  instacart |>
  filter(aisle == "baking ingredients") |>
  group_by(product_name) |>
  summarize(count = n()) |>
  arrange(desc(count)) |>
  head(3) |>
  mutate(aisle = "Baking Ingredients") |>
  relocate(aisle)

dog_food =
  instacart |>
  filter(aisle == "dog food care") |>
  group_by(product_name) |>
  summarize(count = n()) |>
  arrange(desc(count)) |>
  head(3) |>
  mutate(aisle = "Dog Care and Food") |>
  relocate(aisle)

veg_fruit = 
  instacart |>
  filter(aisle == "packaged vegetables fruits") |>
  group_by(product_name) |>
  summarize(count = n()) |>
  arrange(desc(count)) |>
  head(3) |>
  mutate(aisle = "Packaged Vegetable and Fruits") |>
  relocate(aisle)

knitr::kable(
  list(baking, dog_food, veg_fruit),
  caption = "Most Popular Items in Following Aisles"
)

# Simpler way:

instacart |>
  group_by(aisle, product_name) |>
  filter(
    aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")
    ) |>
  summarize(count = n()) |> 
  arrange(desc(count)) |>
  slice(1:3) |>
  knitr::kable(caption = "Most Popular Items in Following Aisles")
```

__4. Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).__

```{r}
instacart |>
  group_by(product_name, order_dow) |>
  summarize(mean_hour = mean(order_hour_of_day)) |>
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) |>
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  ) |>
  rename(
    "Sunday" = "0",
    "Monday" = "1",
    "Tuesday" = "2",
    "Wednesday" = "3",
    "Thursday" = "4",
    "Friday" = "5",
    "Saturaday" = "6"
  ) |>
  knitr::kable(
    caption = "Mean Hour of Day at Which Following Products are Ordered:"
  )
```

## Problem 2:

```{r}
library(p8105.datasets)
data("brfss_smart2010")
```

__1. Data Cleaning:__

- format the data to use appropriate variable names;
- focus on the “Overall Health” topic
- include only responses from “Excellent” to “Poor”
- organize responses as a factor taking levels ordered from “Poor” to “Excellent”

```{r}
brf = 
  brfss_smart2010 |>
  janitor::clean_names() |>
  rename(
    "state" = locationabbr,
    "location" = locationdesc) |>
  filter(
    topic == "Overall Health",
    response %in% c("Excellent", "Very good", "Good", "Fair", "Poor")
  ) |>
  mutate(response = ordered(response, levels = c("Excellent", "Very good", "Good", "Fair", "Poor")))


```

__2. In 2002, which states were observed at 7 or more locations? What about in 2010?__

```{r}
states_2002 =
  brf |>
  group_by(year, state, location) |>
  summarize() |>
  count(state) |>
  filter(
    year == 2002,
    n >= 7
  ) |>
  pull(state)

states_2010 =
  brf |>
  group_by(year, state, location) |>
  summarize() |>
  count(state) |>
  filter(
    year == 2010,
    n >= 7
  ) |>
  pull(state)
```

In 2002, the states that were observed in more than 7 places are `r states_2002`. In 2010, the list changed to `r states_2010`.

__3. Construct a dataset that is limited to Excellent responses, and contains, year, state, and a variable that averages the `data_value` across locations within a state. Make a “spaghetti” plot of this average value over time within a state (that is, make a plot showing a line for each state across years – the geom_line geometry and group aesthetic will help).__

```{r}
brf |>
  filter(response == "Excellent") |>
  group_by(year, state) |>
  summarize(
    mean_value = mean(data_value)
  ) |>
  ggplot(aes(x = year, y = mean_value, group = state, color = state)) +
  geom_point() +
  geom_line() +
  labs(
    title = "Mean Data Value Of Each State Over Time",
    x = "Year",
    y = "Mean Data Value"
  )
```

Different states are grouped and colored. The plot shows averaged `data_value` of all states from year 2002 to 2010.

__4. Make a two-panel plot showing, for the years 2006, and 2010, distribution of data_value for responses (“Poor” to “Excellent”) among locations in NY State.__

```{r}
q4_2006 =
  brf |>
  filter(
    year == 2006,
    state == "NY"
  ) |>
  ggplot(aes(location, data_value, group = response, color = response)) +
  geom_point() +
  geom_line() +
  theme_minimal() +
  scale_x_discrete(guide = guide_axis(angle = 90)) +
  labs(title = "Year 2006", x = "Location in NY", y = "Data Value")


q4_2010 =
  brf |>
  filter(
    year == 2010,
    state == "NY"
  ) |>
  ggplot(aes(location, data_value, group = response, color = response)) +
  geom_point() +
  geom_line() +
  theme_minimal() +
  scale_x_discrete(guide = guide_axis(angle = 90)) +
  labs(title = "Year 2010", x = "Location in NY", y = "Data Value")

gridExtra::grid.arrange(q4_2006, q4_2010, ncol = 2)
```


```{r}
q4_2006 =
  brf |>
  filter(
    year == 2006,
    state == "NY"
  ) |>
  ggplot(aes(x = data_value, y = location, fill = response)) +
  geom_col(position = "dodge") +
  labs(title = "Year 2006", x = "Location in NY", y = "Data Value") +
  theme(legend.position = "right")


q4_2010 =
  brf |>
  filter(
    year == 2010,
    state == "NY"
  ) |>
  ggplot(aes(x = data_value, y = location, fill = response)) +
  geom_col(position = "dodge") +
  labs(title = "Year 2010", x = "Location in NY", y = "Data Value") +
  theme(legend.position = "right")

gridExtra::grid.arrange(q4_2006, q4_2010, nrow = 2)
```

In order to show the data value of responses through counties in NY in 2 different years, I included both scatter plots and bar plots. In both of them, different responses are colored differently, and the plot shows relationship of data value against locations. In 2006, New York County had the highest value in "Excellent" responses, whereas that goes to Westchester County. 

## Problem 3:

__1. Load, tidy, merge, and otherwise organize the data sets. Your final dataset should include all originally observed variables; exclude participants less than 21 years of age, and those with missing demographic data; and encode data with reasonable variable classes (i.e. not numeric, and using factors with the ordering of tables and plots in mind).__

```{r}
demo_df = 
  read_csv("data/nhanes_covar.csv", skip = 4) |>
  janitor::clean_names() |>
  drop_na() |>
  filter(age >= 21) |>
  mutate(sex = recode_factor(sex, '1' = 'male', '2' = 'female', .ordered = T)) |>
  mutate(education = recode_factor(education, 
                           '1' = 'Less than high school',
                           '2' = 'High school equivalent',
                           '3' = 'More than high school',
                           .ordered = T))


accel_df = 
  read_csv("data/nhanes_accel.csv") |>
  janitor::clean_names()

combined_df =
  left_join(demo_df, accel_df)
```

For the demographic data `demo_df`, I dropped all NA values, filtered out entries with age < 21 years old, and recoded sex and education as ordered factors taking human readable values. I then imported the accelerometer dataset and merged it to the demographic dataset. The resulting `combined_df` contains accelerometer reading data from individuals that satisfy the following conditions: greater than or equal to 21 years of age, and no missing demographic data.













